{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd69627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85956a43",
   "metadata": {},
   "source": [
    "- design a neural network to predict 3d pose from 2d coords\n",
    "- options:\n",
    "\n",
    "1. simple MLP:\n",
    "    - input: single frame with camera views concatenated -> (C, J, 2) \n",
    "    - output: 3d coords (J, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f75cb891",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_JOINTS = 20\n",
    "NUM_CAMS = 6\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c19003a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42c86464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for metrics it needs to be (J, 3)\n",
    "\n",
    "# for a test, run a sequence and network will output (60) length vector for each frame in the sequence\n",
    "# (60) -> (20,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48676951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class SimpleMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, activation_function, dropout=0.5):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features = NUM_JOINTS * NUM_CAMS * 3, out_features = 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features = 1024, out_features = 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features= 1024, out_features= NUM_JOINTS * 3)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"Defines a forward pass through the network.\"\"\"\n",
    "        # protect the data and turn all nans to zeros\n",
    "        x[torch.isnan(x)] = 0\n",
    "        # preserve batch dimension, but flatten the rest (C, J, 2) to single vector\n",
    "        x = x.view(x.size(0), -1) \n",
    "\n",
    "        return self.net(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b3efa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28msum\u001b[39m(p.numel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m.parameters())\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f73059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, criterion, loader: DataLoader, device: str):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for dets_2d, gt_3d in loader:\n",
    "            dets_2d = dets_2d.to(device)\n",
    "            gt_3d = gt_3d.to(device)\n",
    "            # flatten the targets before loss computation\n",
    "            gt_3d = gt_3d.view(gt_3d.size(0), -1)\n",
    "\n",
    "            preds = model(dets_2d)  # Forward pass\n",
    "            loss = criterion(preds, gt_3d)  # Compute validation loss\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    val_loss = val_running_loss / len(loader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c136069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model ,criterion, optimizer, train_loader, val_loader, device, epochs=10):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"WE ARE IN EPOCH {epoch}\")\n",
    "        # Training phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for dets_2d, gt_3d in train_loader:\n",
    "\n",
    "            dets_2d = dets_2d.to(device)\n",
    "            gt_3d = gt_3d.to(device)\n",
    "            # flatten 3d gt to match model output\n",
    "            gt_3d = gt_3d.view(gt_3d.size(0), -1)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "\n",
    "            preds = model(dets_2d)  # Forward pass\n",
    "            loss = criterion(preds, gt_3d)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "\n",
    "            running_loss += loss.item()  # Accumulate batch loss\n",
    "\n",
    "        # Calculate epoch training accuracy\n",
    "        # Validation phase (call separate function)\n",
    "        val_loss = evaluate_model(model, criterion,val_loader,device) #CAUTION: See note above\n",
    "\n",
    "        # Print progress\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "            f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "            f\"Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d9e11b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WE ARE IN EPOCH 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x360 and 240x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m model = SimpleMLP(nn.ReLU()).to(device)\n\u001b[32m      6\u001b[39m optimizer = optim.AdamW(model.parameters())\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, criterion, optimizer, train_loader, val_loader, device, epochs)\u001b[39m\n\u001b[32m     17\u001b[39m gt_3d = gt_3d.view(gt_3d.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     19\u001b[39m optimizer.zero_grad()  \u001b[38;5;66;03m# Clear previous gradients\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets_2d\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m     23\u001b[39m loss = criterion(preds, gt_3d)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m     24\u001b[39m loss.backward()  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intelligent_earth/cheetah-motion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intelligent_earth/cheetah-motion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mSimpleMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# preserve batch dimension, but flatten the rest (C, J, 2) to single vector\u001b[39;00m\n\u001b[32m     24\u001b[39m x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m) \n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intelligent_earth/cheetah-motion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intelligent_earth/cheetah-motion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intelligent_earth/cheetah-motion/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:253\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intelligent_earth/cheetah-motion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intelligent_earth/cheetah-motion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intelligent_earth/cheetah-motion/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (32x360 and 240x1024)"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "from data_loader import get_data_loaders\n",
    "\n",
    "train_loader, val_loader, test_loader = get_data_loaders()\n",
    "model = SimpleMLP(nn.ReLU()).to(device)\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "\n",
    "train_model(model, nn.MSELoss(), optimizer, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501cb28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
