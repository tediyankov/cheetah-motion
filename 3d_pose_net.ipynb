{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9cf16e5-4020-4871-9157-715c9593ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-23 11:27:07.956887: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-23 11:27:08.132068: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2026-02-23 11:27:08.132106: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2026-02-23 11:27:09.594918: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2026-02-23 11:27:09.595004: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2026-02-23 11:27:09.595015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tables\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7da534-71da-4cee-9bb7-c56181443b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure TF1-style behavior (matches the original repo)\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# ----------------------------\n",
    "# LinearModel (adapted)\n",
    "# ----------------------------\n",
    "def kaiming(shape, dtype=None, partition_info=None):\n",
    "    \"\"\"Kaiming initialization function compatible with tf.get_variable initializer.\n",
    "    The original repo used a callable initializer; we keep the same behavior here.\n",
    "    \"\"\"\n",
    "    # shape[0] used in original (fan_in). We guard for shape being tuple/list.\n",
    "    fan_in = float(shape[0]) if len(shape) > 0 else 1.0\n",
    "    std = np.sqrt(2.0 / fan_in)\n",
    "    return tf.random.truncated_normal(shape, dtype=dtype) * std\n",
    "\n",
    "\n",
    "class LinearModel(object):\n",
    "    \"\"\"Linear + ReLU model compatible with the original H36M linear model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 linear_size=1024,\n",
    "                 num_layers=2,\n",
    "                 residual=False,\n",
    "                 batch_norm=False,\n",
    "                 max_norm=False,\n",
    "                 batch_size=64,\n",
    "                 learning_rate=1e-3,\n",
    "                 summaries_dir=None,\n",
    "                 predict_14=False,\n",
    "                 dtype=tf.float32):\n",
    "        \"\"\"\n",
    "        Args mirror the original implementation. This version assumes the input\n",
    "        is 16 joints x,y => 32 dims (HUMAN_2D_SIZE).\n",
    "        \"\"\"\n",
    "\n",
    "        # Constants same as original repo\n",
    "        self.HUMAN_2D_SIZE = 16 * 2\n",
    "        self.HUMAN_3D_SIZE = 14 * 3 if predict_14 else 16 * 3\n",
    "\n",
    "        self.input_size = self.HUMAN_2D_SIZE\n",
    "        self.output_size = self.HUMAN_3D_SIZE\n",
    "\n",
    "        # TF placeholders\n",
    "        self.isTraining = tf.compat.v1.placeholder(tf.bool, name=\"isTrainingflag\")\n",
    "        self.dropout_keep_prob = tf.compat.v1.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Summary writers (optional)\n",
    "        if summaries_dir is not None:\n",
    "            os.makedirs(os.path.join(summaries_dir, 'train'), exist_ok=True)\n",
    "            os.makedirs(os.path.join(summaries_dir, 'test'), exist_ok=True)\n",
    "            self.train_writer = tf.compat.v1.summary.FileWriter(os.path.join(summaries_dir, 'train'))\n",
    "            self.test_writer = tf.compat.v1.summary.FileWriter(os.path.join(summaries_dir, 'test'))\n",
    "        else:\n",
    "            self.train_writer = None\n",
    "            self.test_writer = None\n",
    "\n",
    "        # Params\n",
    "        self.linear_size = linear_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False, dtype=dtype, name=\"learning_rate\")\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "        # Learning rate decay (same defaults as original)\n",
    "        decay_steps = 100000\n",
    "        decay_rate = 0.96\n",
    "        self.learning_rate = tf.compat.v1.train.exponential_decay(self.learning_rate, self.global_step, decay_steps, decay_rate)\n",
    "\n",
    "        # Inputs placeholders\n",
    "        with tf.compat.v1.variable_scope(\"inputs\"):\n",
    "            enc_in = tf.compat.v1.placeholder(dtype, shape=[None, self.input_size], name=\"enc_in\")\n",
    "            dec_out = tf.compat.v1.placeholder(dtype, shape=[None, self.output_size], name=\"dec_out\")\n",
    "            self.encoder_inputs = enc_in\n",
    "            self.decoder_outputs = dec_out\n",
    "\n",
    "        # Build model (linear + relu blocks)\n",
    "        with tf.compat.v1.variable_scope(\"linear_model\"):\n",
    "            # First linear layer\n",
    "            w1 = tf.compat.v1.get_variable(name=\"w1\", initializer=kaiming, shape=[self.HUMAN_2D_SIZE, linear_size], dtype=dtype)\n",
    "            b1 = tf.compat.v1.get_variable(name=\"b1\", initializer=kaiming, shape=[linear_size], dtype=dtype)\n",
    "            if max_norm:\n",
    "                w1 = tf.clip_by_norm(w1, 1)\n",
    "            y3 = tf.matmul(enc_in, w1) + b1\n",
    "\n",
    "            if batch_norm:\n",
    "                # tf.compat.v1.layers.batch_normalization\n",
    "                y3 = tf.compat.v1.layers.batch_normalization(y3, training=self.isTraining, name=\"batch_normalization\")\n",
    "            y3 = tf.nn.relu(y3)\n",
    "            y3 = tf.nn.dropout(y3, rate=1.0 - self.dropout_keep_prob)  # TF2-style dropout with rate\n",
    "\n",
    "            # Two-linear blocks (num_layers times)\n",
    "            for idx in range(num_layers):\n",
    "                y3 = self.two_linear(y3, linear_size, residual, self.dropout_keep_prob, max_norm, batch_norm, dtype, idx)\n",
    "\n",
    "            # Final linear -> output\n",
    "            w4 = tf.compat.v1.get_variable(name=\"w4\", initializer=kaiming, shape=[linear_size, self.HUMAN_3D_SIZE], dtype=dtype)\n",
    "            b4 = tf.compat.v1.get_variable(name=\"b4\", initializer=kaiming, shape=[self.HUMAN_3D_SIZE], dtype=dtype)\n",
    "            if max_norm:\n",
    "                w4 = tf.clip_by_norm(w4, 1)\n",
    "            y = tf.matmul(y3, w4) + b4\n",
    "\n",
    "        # Outputs & loss\n",
    "        self.outputs = y\n",
    "        self.loss = tf.reduce_mean(tf.square(y - dec_out))\n",
    "        self.loss_summary = tf.compat.v1.summary.scalar('loss/loss', self.loss)\n",
    "\n",
    "        # Error in mm placeholder for TB logging (keeps API similar)\n",
    "        self.err_mm = tf.compat.v1.placeholder(tf.float32, name=\"error_mm\")\n",
    "        self.err_mm_summary = tf.compat.v1.summary.scalar(\"loss/error_mm\", self.err_mm)\n",
    "\n",
    "        # Optimizer and updates\n",
    "        opt = tf.compat.v1.train.AdamOptimizer(self.learning_rate)\n",
    "        update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            gradients = opt.compute_gradients(self.loss)\n",
    "            self.gradients = [[] if i is None else i for i in gradients]\n",
    "            self.updates = opt.apply_gradients(gradients, global_step=self.global_step)\n",
    "\n",
    "        self.learning_rate_summary = tf.compat.v1.summary.scalar('learning_rate/learning_rate', self.learning_rate)\n",
    "        self.saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables(), max_to_keep=10)\n",
    "\n",
    "    def two_linear(self, xin, linear_size, residual, dropout_keep_prob, max_norm, batch_norm, dtype, idx):\n",
    "        \"\"\"Two linear layers with optional residual (mirrors original).\"\"\"\n",
    "        with tf.compat.v1.variable_scope(\"two_linear_%d\" % idx):\n",
    "            input_size = int(xin.get_shape()[1])\n",
    "\n",
    "            # Linear 1\n",
    "            w2 = tf.compat.v1.get_variable(name=\"w2_%d\" % idx, initializer=kaiming, shape=[input_size, linear_size], dtype=dtype)\n",
    "            b2 = tf.compat.v1.get_variable(name=\"b2_%d\" % idx, initializer=kaiming, shape=[linear_size], dtype=dtype)\n",
    "            if max_norm:\n",
    "                w2 = tf.clip_by_norm(w2, 1)\n",
    "            y = tf.matmul(xin, w2) + b2\n",
    "            if batch_norm:\n",
    "                y = tf.compat.v1.layers.batch_normalization(y, training=self.isTraining, name=\"batch_norm1_%d\" % idx)\n",
    "            y = tf.nn.relu(y)\n",
    "            y = tf.nn.dropout(y, rate=1.0 - dropout_keep_prob)\n",
    "\n",
    "            # Linear 2\n",
    "            w3 = tf.compat.v1.get_variable(name=\"w3_%d\" % idx, initializer=kaiming, shape=[linear_size, linear_size], dtype=dtype)\n",
    "            b3 = tf.compat.v1.get_variable(name=\"b3_%d\" % idx, initializer=kaiming, shape=[linear_size], dtype=dtype)\n",
    "            if max_norm:\n",
    "                w3 = tf.clip_by_norm(w3, 1)\n",
    "            y = tf.matmul(y, w3) + b3\n",
    "            if batch_norm:\n",
    "                y = tf.compat.v1.layers.batch_normalization(y, training=self.isTraining, name=\"batch_norm2_%d\" % idx)\n",
    "            y = tf.nn.relu(y)\n",
    "            y = tf.nn.dropout(y, rate=1.0 - dropout_keep_prob)\n",
    "\n",
    "            # Residual (original only adds input every 2 blocks; we keep behavior same as input param)\n",
    "            if residual:\n",
    "                y = xin + y\n",
    "\n",
    "            return y\n",
    "\n",
    "    def step(self, session, encoder_inputs, decoder_outputs, dropout_keep_prob, isTraining=True):\n",
    "        \"\"\"Run a training or validation step. Returns similar tuple to original repo.\"\"\"\n",
    "\n",
    "        input_feed = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.decoder_outputs: decoder_outputs,\n",
    "            self.isTraining: isTraining,\n",
    "            self.dropout_keep_prob: dropout_keep_prob\n",
    "        }\n",
    "\n",
    "        if isTraining:\n",
    "            output_feed = [self.updates, self.loss, self.loss_summary, self.learning_rate_summary, self.outputs]\n",
    "            outputs = session.run(output_feed, input_feed)\n",
    "            # returns (loss, loss_summary, lr_summary, outputs)\n",
    "            return outputs[1], outputs[2], outputs[3], outputs[4]\n",
    "        else:\n",
    "            output_feed = [self.loss, self.loss_summary, self.outputs]\n",
    "            outputs = session.run(output_feed, input_feed)\n",
    "            return outputs[0], outputs[1], outputs[2]\n",
    "\n",
    "\n",
    "    # A helper batch maker for already-prepared numpy arrays\n",
    "    def get_all_batches_from_arrays(self, X, Y, training=True):\n",
    "        \"\"\"Split X,Y arrays into batches matching self.batch_size.\n",
    "           Returns lists of batches: (X_batches, Y_batches)\"\"\"\n",
    "        assert X.shape[0] == Y.shape[0], \"X and Y must have same number of rows\"\n",
    "        n = X.shape[0]\n",
    "        if training:\n",
    "            perm = np.random.permutation(n)\n",
    "            X = X[perm]\n",
    "            Y = Y[perm]\n",
    "        # Trim extras so divisible by batch_size\n",
    "        n_extra = n % self.batch_size\n",
    "        if n_extra > 0:\n",
    "            X = X[:-n_extra]\n",
    "            Y = Y[:-n_extra]\n",
    "        n_batches = X.shape[0] // self.batch_size\n",
    "        if n_batches == 0:\n",
    "            raise ValueError(\"Not enough examples for a single batch. Increase data or reduce batch_size.\")\n",
    "        X_batches = np.split(X, n_batches)\n",
    "        Y_batches = np.split(Y, n_batches)\n",
    "        return X_batches, Y_batches\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Training loop helper\n",
    "# ----------------------------\n",
    "def train_model_on_arrays(X_train, Y_train, X_val, Y_val,\n",
    "                          linear_size=1024, num_layers=2, residual=False,\n",
    "                          batch_norm=False, max_norm=False, batch_size=64,\n",
    "                          learning_rate=1e-3, epochs=10, dropout=1.0,\n",
    "                          summaries_dir=None, predict_14=False, use_gpu=True,\n",
    "                          restore_checkpoint=None, save_dir='experiments'):\n",
    "    \"\"\"\n",
    "    Train the LinearModel on arrays. This is the simple analog of the original train() flow.\n",
    "    \"\"\"\n",
    "    device_count = {\"GPU\": 0} if not use_gpu else {\"GPU\": 1}\n",
    "    config = tf.compat.v1.ConfigProto(device_count=device_count, allow_soft_placement=True)\n",
    "\n",
    "    # Prepare training directory\n",
    "    train_dir = save_dir\n",
    "    summaries_dir = summaries_dir or os.path.join(train_dir, \"log\")\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(summaries_dir, exist_ok=True)\n",
    "\n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "    # Create model\n",
    "    model = LinearModel(\n",
    "        linear_size=linear_size,\n",
    "        num_layers=num_layers,\n",
    "        residual=residual,\n",
    "        batch_norm=batch_norm,\n",
    "        max_norm=max_norm,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        summaries_dir=summaries_dir,\n",
    "        predict_14=predict_14,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "\n",
    "    # Initialize or restore\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    if restore_checkpoint:\n",
    "        ckpt = tf.train.get_checkpoint_state(restore_checkpoint)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    train_writer = model.train_writer\n",
    "\n",
    "    # Training loop\n",
    "    current_step = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        X_batches, Y_batches = model.get_all_batches_from_arrays(X_train, Y_train, training=True)\n",
    "        nbatches = len(X_batches)\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(nbatches):\n",
    "            enc_in = X_batches[i]\n",
    "            dec_out = Y_batches[i]\n",
    "            step_loss, loss_summary, lr_summary, _ = model.step(sess, enc_in, dec_out, dropout_keep_prob=dropout, isTraining=True)\n",
    "\n",
    "            if train_writer is not None and (current_step % 100 == 0):\n",
    "                train_writer.add_summary(loss_summary, current_step)\n",
    "                train_writer.add_summary(lr_summary, current_step)\n",
    "\n",
    "            epoch_loss += step_loss\n",
    "            current_step += 1\n",
    "\n",
    "        epoch_loss /= nbatches\n",
    "        print(f\"  Train loss avg: {epoch_loss:.6f}  (time: {time.time()-start_time:.2f}s)\")\n",
    "\n",
    "        # Validation\n",
    "        Xv_batches, Yv_batches = model.get_all_batches_from_arrays(X_val, Y_val, training=False)\n",
    "        val_loss = 0.0\n",
    "        for i in range(len(Xv_batches)):\n",
    "            vl, vs, preds = model.step(sess, Xv_batches[i], Yv_batches[i], dropout_keep_prob=1.0, isTraining=False)\n",
    "            val_loss += vl\n",
    "        val_loss /= len(Xv_batches)\n",
    "        print(f\"  Val loss avg:   {val_loss:.6f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        ckpt_name = os.path.join(train_dir, 'checkpoint')\n",
    "        model.saver.save(sess, ckpt_name, global_step=current_step)\n",
    "        print(f\"  Saved checkpoint at step {current_step}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    # RETURN the open session (caller must close it when done)\n",
    "    return model, sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8cea3e7-caaf-4bd7-8822-23319feaf6c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1771347372.478110    2157 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1771347372.479353    2157 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  Train loss avg: 1.096895  (time: 1.26s)\n",
      "  Val loss avg:   1.013701\n",
      "  Saved checkpoint at step 32\n",
      "Epoch 2/3\n",
      "  Train loss avg: 1.002927  (time: 0.98s)\n",
      "  Val loss avg:   1.013460\n",
      "  Saved checkpoint at step 64\n",
      "Epoch 3/3\n",
      "  Train loss avg: 0.998458  (time: 0.82s)\n",
      "  Val loss avg:   1.014558\n",
      "  Saved checkpoint at step 96\n",
      "Training finished.\n",
      "Example validation loss: 1.0390238\n",
      "Predictions shape: (64, 48)\n"
     ]
    }
   ],
   "source": [
    "# Clear any existing graph (VERY important in notebooks)\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# ----------------------------\n",
    "# Synthetic Example Data\n",
    "# ----------------------------\n",
    "N_train = 2048\n",
    "N_val = 512\n",
    "\n",
    "batch_size = 64\n",
    "predict_14 = False\n",
    "\n",
    "input_dim = 16 * 2\n",
    "output_dim = 14*3 if predict_14 else 16*3\n",
    "\n",
    "# Generate synthetic data\n",
    "X_train = np.random.randn(N_train, input_dim).astype(np.float32)\n",
    "Y_train = np.random.randn(N_train, output_dim).astype(np.float32)\n",
    "\n",
    "X_val = np.random.randn(N_val, input_dim).astype(np.float32)\n",
    "Y_val = np.random.randn(N_val, output_dim).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# Train\n",
    "# ----------------------------\n",
    "model, sess = train_model_on_arrays(\n",
    "    X_train, Y_train,\n",
    "    X_val, Y_val,\n",
    "    linear_size=1024,\n",
    "    num_layers=2,\n",
    "    residual=False,\n",
    "    batch_norm=False,\n",
    "    max_norm=False,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=1e-3,\n",
    "    epochs=3,\n",
    "    dropout=0.9,\n",
    "    summaries_dir=\"./logs_example\",\n",
    "    predict_14=predict_14,\n",
    "    use_gpu=True,\n",
    "    save_dir=\"./example_experiments\"\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate on ONE FULL BATCH\n",
    "# ----------------------------\n",
    "# Make sure we pass exactly one batch_size worth of samples\n",
    "Xb = X_val[:batch_size]\n",
    "Yb = Y_val[:batch_size]\n",
    "\n",
    "loss_val, summary, preds = model.step(\n",
    "    sess,\n",
    "    Xb,\n",
    "    Yb,\n",
    "    dropout_keep_prob=1.0,\n",
    "    isTraining=False\n",
    ")\n",
    "\n",
    "print(\"Example validation loss:\", loss_val)\n",
    "print(\"Predictions shape:\", preds.shape)\n",
    "\n",
    "# Close session when completely finished\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "974e9b6d-9d68-4d5c-b63b-ee8a81f1cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scene(scene_json_path: Path) -> list:\n",
    "    with open(scene_json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    cameras = []\n",
    "\n",
    "    for cam in data[\"cameras\"]:\n",
    "        # Intrinsics\n",
    "        K = np.array(cam[\"k\"], dtype=np.float64).reshape(3, 3)\n",
    "\n",
    "        # Distortion (not needed for DLT, but useful to keep)\n",
    "        D = np.array(cam[\"d\"], dtype=np.float64)\n",
    "\n",
    "        # Extrinsics\n",
    "        R = np.array(cam[\"r\"], dtype=np.float64).reshape(3, 3)\n",
    "        t = np.array(cam[\"t\"], dtype=np.float64).reshape(3, 1)\n",
    "\n",
    "        # Projection matrix: P = K [R | t]\n",
    "        Rt = np.hstack([R, t])   # 3x4\n",
    "        P = K @ Rt               # 3x4\n",
    "\n",
    "        cameras.append({\n",
    "            \"K\": K,\n",
    "            \"D\": D,\n",
    "            \"R\": R,\n",
    "            \"t\": t,\n",
    "            \"P\": P\n",
    "        })\n",
    "\n",
    "    return cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f88fe2-9382-44e8-a1e3-35a03015be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for 2d joint detections and camera projection matrices\n",
    "\n",
    "cam3 = pd.read_hdf(\"/gws/nopw/j04/iecdt/cheetah/2017_08_29/bottom/phantom/flick2/filtered_2D/cam3DLC_resnet152_CheetahOct14shuffle1_500000.h5\")\n",
    "cam4 = pd.read_hdf(\"/gws/nopw/j04/iecdt/cheetah/2017_08_29/bottom/phantom/flick2/filtered_2D/cam4DLC_resnet152_CheetahOct14shuffle1_500000.h5\")\n",
    "cam5 = pd.read_hdf(\"/gws/nopw/j04/iecdt/cheetah/2017_08_29/bottom/phantom/flick2/filtered_2D/cam5DLC_resnet152_CheetahOct14shuffle1_500000.h5\")\n",
    "cam6 = pd.read_hdf(\"/gws/nopw/j04/iecdt/cheetah/2017_08_29/bottom/phantom/flick2/filtered_2D/cam6DLC_resnet152_CheetahOct14shuffle1_500000.h5\")\n",
    "\n",
    "cams = [cam3, cam4, cam5, cam6]\n",
    "\n",
    "cam_path = Path(\"/gws/nopw/j04/iecdt/cheetah/2017_08_29/bottom/extrinsic_calib/4_cam_scene_sba.json\")\n",
    "\n",
    "cameras = load_scene(cam_path)\n",
    "\n",
    "Ps = [cam[\"P\"] for cam in cameras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06442216-ef11-49e2-9a8b-48255a1276f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "def _find_column_triplet(df: pd.DataFrame, bodypart: str) -> Tuple[Tuple, Tuple, Tuple]:\n",
    "    \"\"\"\n",
    "    Find the (x,y,likelihood) column tuples for a given bodypart in a DLC-style MultiIndex df.\n",
    "    Returns (xcol, ycol, lcol) as column-key tuples suitable for df[...] indexing.\n",
    "    Raises KeyError if not found.\n",
    "    \"\"\"\n",
    "    # Accept both 2- or 3-level column index shapes, but DLC usually has 3-level: (scorer, bodypart, coord)\n",
    "    cols = list(df.columns)\n",
    "    xcol = ycol = lcol = None\n",
    "    for c in cols:\n",
    "        # treat c as tuple-like\n",
    "        if len(c) >= 2 and c[1] == bodypart:\n",
    "            coord = c[-1]  # assume final level is 'x','y','likelihood'\n",
    "            if coord == 'x':\n",
    "                xcol = c\n",
    "            elif coord == 'y':\n",
    "                ycol = c\n",
    "            elif coord in ('likelihood','prob','score','p'):\n",
    "                lcol = c\n",
    "    if xcol is None or ycol is None:\n",
    "        raise KeyError(f\"couldn't find x/y columns for bodypart '{bodypart}'\")\n",
    "    # if likelihood missing, create a dummy column tuple (we'll fill with NaNs later)\n",
    "    if lcol is None:\n",
    "        # create a fake tuple with 'likelihood' appended (will error if used for indexing)\n",
    "        # We prefer returning None to indicate missing\n",
    "        lcol = None\n",
    "    return xcol, ycol, lcol\n",
    "\n",
    "def get_joint_from_df(df: pd.DataFrame, bodypart: str, frame: int) -> Tuple[Optional[float], Optional[float], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Return (u, v, likelihood) for a given bodypart at a specific frame index from a single camera DataFrame.\n",
    "    If likelihood column is missing, returns np.nan for likelihood.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        xcol, ycol, lcol = _find_column_triplet(df, bodypart)\n",
    "    except KeyError:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    u = df.at[frame, xcol] if xcol in df.columns else np.nan\n",
    "    v = df.at[frame, ycol] if ycol in df.columns else np.nan\n",
    "    if lcol is not None and lcol in df.columns:\n",
    "        l = df.at[frame, lcol]\n",
    "    else:\n",
    "        l = np.nan\n",
    "    # ensure numeric (could be pandas Series scalar)\n",
    "    return float(u) if pd.notna(u) else np.nan, float(v) if pd.notna(v) else np.nan, float(l) if pd.notna(l) else np.nan\n",
    "\n",
    "def triangulate_point_dlt(us_vs: List[Tuple[float,float]], Ps: List[np.ndarray]) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    us_vs: list of (u,v) observations (pixel coords) for the same 3D point across cameras\n",
    "    Ps: corresponding list of 3x4 camera projection matrices (same order)\n",
    "    Returns: 3-element np.array [X, Y, Z] (float) on success, or None if failure.\n",
    "    \"\"\"\n",
    "    if len(us_vs) < 2:\n",
    "        return None\n",
    "\n",
    "    # Build A (2*C x 4)\n",
    "    A_rows = []\n",
    "    for (u, v), P in zip(us_vs, Ps):\n",
    "        p1 = P[0, :]  # 1x4\n",
    "        p2 = P[1, :]\n",
    "        p3 = P[2, :]\n",
    "        A_rows.append(u * p3 - p1)\n",
    "        A_rows.append(v * p3 - p2)\n",
    "    A = np.vstack(A_rows)  # shape (2C, 4)\n",
    "\n",
    "    # Solve by SVD: A x = 0 -> x is last col of V (Vt[-1])\n",
    "    try:\n",
    "        _, _, Vt = np.linalg.svd(A)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return None\n",
    "    X = Vt[-1, :]  # last row of Vt is last column of V\n",
    "    if np.isclose(X[-1], 0):\n",
    "        # cannot homogenize; ill-conditioned\n",
    "        return None\n",
    "    X = X / X[-1]\n",
    "    return X[:3]  # X, Y, Z\n",
    "\n",
    "def triangulate_frame_all_joints(\n",
    "    cams_dfs: List[pd.DataFrame],\n",
    "    P_matrices: List[np.ndarray],\n",
    "    frame_idx: int,\n",
    "    conf_thresh: float = 0.6,\n",
    "    bodyparts: Optional[List[str]] = None\n",
    ") -> Dict[str, Optional[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Triangulate each bodypart for a single frame.\n",
    "    - cams_dfs: list of pandas DataFrames, one per camera (same ordering as P_matrices)\n",
    "    - P_matrices: list of 3x4 numpy arrays\n",
    "    - frame_idx: integer row index in the DataFrames (same frame indexing across cams)\n",
    "    - conf_thresh: min likelihood to count a detection\n",
    "    - bodyparts: optional list of bodypart names to process; if None, infer from first df\n",
    "    Returns: dict mapping bodypart -> np.array([X,Y,Z]) or None if triangulation not possible.\n",
    "    \"\"\"\n",
    "    if len(cams_dfs) != len(P_matrices):\n",
    "        raise ValueError(\"number of cameras (dfs) must match number of projection matrices\")\n",
    "\n",
    "    # infer bodyparts from first DataFrame if not supplied\n",
    "    if bodyparts is None:\n",
    "        first_df = cams_dfs[0]\n",
    "        # find second level values assuming MultiIndex with bodypart as level 1\n",
    "        if isinstance(first_df.columns, pd.MultiIndex) and first_df.columns.nlevels >= 2:\n",
    "            bodyparts = list(dict.fromkeys(first_df.columns.get_level_values(1)))  # preserve order, unique\n",
    "        else:\n",
    "            # fallback: can't infer\n",
    "            raise ValueError(\"Cannot infer bodyparts from the DataFrame columns. Provide bodyparts argument.\")\n",
    "\n",
    "    results = {}\n",
    "    for bp in bodyparts:\n",
    "        obs = []   # list of (u, v)\n",
    "        Ps = []    # corresponding projection matrices\n",
    "        for df, P in zip(cams_dfs, P_matrices):\n",
    "            u, v, l = get_joint_from_df(df, bp, frame_idx)\n",
    "            # treat NaN or small likelihood as missing\n",
    "            if np.isfinite(u) and np.isfinite(v) and (np.isfinite(l) and l >= conf_thresh):\n",
    "                obs.append((u, v))\n",
    "                Ps.append(P)\n",
    "            # if there is no likelihood column (l is NaN), you may optionally still use the point:\n",
    "            # elif np.isfinite(u) and np.isfinite(v) and np.isnan(l):\n",
    "            #     obs.append((u,v)); Ps.append(P)\n",
    "        if len(obs) < 2:\n",
    "            results[bp] = None\n",
    "        else:\n",
    "            X = triangulate_point_dlt(obs, Ps)\n",
    "            results[bp] = X  # np.array([X,Y,Z]) or None\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "505d21c2-ce8c-449a-9e62-2e2ad91e8927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bodyparts (order): ['r_eye', 'l_eye', 'r_shoulder', 'r_front_knee', 'r_front_ankle', 'r_front_paw', 'spine', 'r_hip', 'r_back_knee', 'r_back_ankle', 'r_back_paw', 'tail1', 'tail2', 'l_shoulder', 'l_front_knee', 'l_front_ankle']\n",
      "Frames available (min across cams): 321\n",
      "Collected 0 samples (frames) with 16 joints each.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No training samples found. Try lowering conf_thresh or min_frac_joints.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples (frames) with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_joints_req\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m joints each.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo training samples found. Try lowering conf_thresh or min_frac_joints.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# --- Train/Val split (random) ---\u001b[39;00m\n\u001b[1;32m     83\u001b[0m N \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No training samples found. Try lowering conf_thresh or min_frac_joints."
     ]
    }
   ],
   "source": [
    "# Reset graph (important for repeated runs in notebooks)\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# --- User-configurable options ---\n",
    "conf_thresh = 0.6          # minimum detection confidence to accept 2D points\n",
    "min_frac_joints = 1.00     # fraction of required joints that must be present in a frame (1.0 -> require all)\n",
    "use_camera_index = 0       # which camera DataFrame to use as the input 2D view (0 = first camera in `cams`)\n",
    "predict_14 = False         # match model setting (False -> predict 16 joints*3)\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "dropout = 0.9\n",
    "\n",
    "# --- Infer bodyparts list (take first 16 bodyparts from the first camera DF by default) ---\n",
    "first_df = cams[use_camera_index]\n",
    "if not (isinstance(first_df.columns, pd.MultiIndex) and first_df.columns.nlevels >= 2):\n",
    "    raise RuntimeError(\"Expected DLC-style MultiIndex columns (scorer, bodypart, coord) in camera DataFrame.\")\n",
    "\n",
    "all_bodyparts = list(dict.fromkeys(first_df.columns.get_level_values(1)))\n",
    "# Ensure we pick exactly 16 bodyparts for this model\n",
    "n_joints_req = 16\n",
    "if len(all_bodyparts) < n_joints_req:\n",
    "    raise RuntimeError(f\"Only found {len(all_bodyparts)} bodyparts; model expects {n_joints_req}.\")\n",
    "bodyparts = all_bodyparts[:n_joints_req]\n",
    "print(\"Using bodyparts (order):\", bodyparts)\n",
    "\n",
    "# --- Build dataset (iterate frames, triangulate per-frame) ---\n",
    "n_frames = min(df.shape[0] for df in cams)  # use smallest frame count across cameras\n",
    "print(\"Frames available (min across cams):\", n_frames)\n",
    "\n",
    "X_list = []\n",
    "Y_list = []\n",
    "frames_used = []\n",
    "\n",
    "for frame_idx in range(n_frames):\n",
    "    # Triangulate all joints for this frame (uses DLT function you already have)\n",
    "    tri3d = triangulate_frame_all_joints(cams, Ps, frame_idx, conf_thresh=conf_thresh, bodyparts=bodyparts)\n",
    "\n",
    "    uvs = []\n",
    "    valid_mask = []\n",
    "    for bp in bodyparts:\n",
    "        # get 2D from chosen camera\n",
    "        u, v, l = get_joint_from_df(cams[use_camera_index], bp, frame_idx)\n",
    "        # require finite coords and confidence threshold\n",
    "        ok_2d = np.isfinite(u) and np.isfinite(v) and (np.isfinite(l) and l >= conf_thresh)\n",
    "        ok_3d = (tri3d.get(bp) is not None) and (tri3d.get(bp) is not None and np.all(np.isfinite(tri3d.get(bp))))\n",
    "        if ok_2d and ok_3d:\n",
    "            uvs.append((u, v))\n",
    "            valid_mask.append(True)\n",
    "        else:\n",
    "            uvs.append((np.nan, np.nan))\n",
    "            valid_mask.append(False)\n",
    "\n",
    "    n_valid = sum(valid_mask)\n",
    "    if n_valid / float(n_joints_req) >= min_frac_joints:\n",
    "        # require all joints present by default; change min_frac_joints if you want a looser filter\n",
    "        # build input vector (flatten u,v for each joint in order)\n",
    "        X_vec = np.array([coord for uv in uvs for coord in uv], dtype=np.float32)  # length = n_joints_req * 2\n",
    "\n",
    "        # build target vector (flatten X,Y,Z for each joint in order)\n",
    "        Y_vec = np.zeros((n_joints_req * 3,), dtype=np.float32)\n",
    "        for i, bp in enumerate(bodyparts):\n",
    "            pt3 = tri3d.get(bp)\n",
    "            if pt3 is None:\n",
    "                # should not happen due to min_frac_joints check; but set NaN to flag if it does\n",
    "                Y_vec[3*i:3*i+3] = np.array([np.nan, np.nan, np.nan], dtype=np.float32)\n",
    "            else:\n",
    "                Y_vec[3*i:3*i+3] = np.array(pt3, dtype=np.float32)\n",
    "        # final sanity: ensure no NaNs in target/input\n",
    "        if np.all(np.isfinite(X_vec)) and np.all(np.isfinite(Y_vec)):\n",
    "            X_list.append(X_vec)\n",
    "            Y_list.append(Y_vec)\n",
    "            frames_used.append(frame_idx)\n",
    "    # else skip this frame\n",
    "\n",
    "X = np.stack(X_list, axis=0) if len(X_list) > 0 else np.empty((0, n_joints_req*2), dtype=np.float32)\n",
    "Y = np.stack(Y_list, axis=0) if len(Y_list) > 0 else np.empty((0, n_joints_req*3), dtype=np.float32)\n",
    "\n",
    "print(f\"Collected {X.shape[0]} samples (frames) with {n_joints_req} joints each.\")\n",
    "if X.shape[0] == 0:\n",
    "    raise RuntimeError(\"No training samples found. Try lowering conf_thresh or min_frac_joints.\")\n",
    "\n",
    "# --- Train/Val split (random) ---\n",
    "N = X.shape[0]\n",
    "perm = np.random.permutation(N)\n",
    "train_frac = 0.8\n",
    "n_train = int(train_frac * N)\n",
    "train_idx = perm[:n_train]\n",
    "val_idx = perm[n_train:]\n",
    "\n",
    "X_train = X[train_idx]\n",
    "Y_train = Y[train_idx]\n",
    "X_val = X[val_idx]\n",
    "Y_val = Y[val_idx]\n",
    "\n",
    "print(\"Train/Val shapes:\", X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)\n",
    "\n",
    "# --- Cast to float32 (already done) and check dims match model expected dims ---\n",
    "input_dim = n_joints_req * 2\n",
    "output_dim = (14*3 if predict_14 else 16*3)\n",
    "if X_train.shape[1] != input_dim:\n",
    "    raise RuntimeError(f\"Input dim mismatch: X has {X_train.shape[1]} but model expects {input_dim}.\")\n",
    "if Y_train.shape[1] != output_dim:\n",
    "    raise RuntimeError(f\"Output dim mismatch: Y has {Y_train.shape[1]} but model expects {output_dim}.\")\n",
    "\n",
    "# --- Train the model using the existing helper ---\n",
    "model, sess = train_model_on_arrays(\n",
    "    X_train, Y_train,\n",
    "    X_val, Y_val,\n",
    "    linear_size=1024,\n",
    "    num_layers=2,\n",
    "    residual=False,\n",
    "    batch_norm=False,\n",
    "    max_norm=False,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=1e-3,\n",
    "    epochs=epochs,\n",
    "    dropout=dropout,\n",
    "    summaries_dir=\"./logs_realdata\",\n",
    "    predict_14=predict_14,\n",
    "    use_gpu=True,\n",
    "    save_dir=\"./realdata_experiments\"\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate on ONE FULL BATCH from validation (same size as batch_size)\n",
    "# ----------------------------\n",
    "if X_val.shape[0] < batch_size:\n",
    "    print(\"Warning: validation set has fewer than batch_size samples; using all validation samples for one evaluation batch.\")\n",
    "    Xb = X_val\n",
    "    Yb = Y_val\n",
    "else:\n",
    "    Xb = X_val[:batch_size]\n",
    "    Yb = Y_val[:batch_size]\n",
    "\n",
    "loss_val, summary, preds = model.step(\n",
    "    sess,\n",
    "    Xb,\n",
    "    Yb,\n",
    "    dropout_keep_prob=1.0,\n",
    "    isTraining=False\n",
    ")\n",
    "\n",
    "print(\"Validation loss:\", loss_val)\n",
    "print(\"Predictions shape:\", preds.shape)\n",
    "\n",
    "# Keep session open for further evaluation / saving outside this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0d5fa-1f24-4ace-ac9c-aaebde1a5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pose_tf_final)",
   "language": "python",
   "name": "pose_tf_final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
