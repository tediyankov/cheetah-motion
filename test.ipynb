{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4092b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict\n",
    "import pickle\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a5b3184",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_path = Path('/gws/nopw/j04/iecdt/cheetah/2017_08_29/bottom/phantom/flick2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f0a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sample:\n",
    "    # TODO: include the path to the data so we know where the frame came from in the same way that frame_idx was included\n",
    "    \"\"\"\n",
    "    Represents one frame's data for a particular camera inside a Sequence.\n",
    "    detections_2D: np.ndarray, shape (C=6, J, 3) where C isnumber of cameras\n",
    "            J is number of bodyparts, and the last dim is (u, v, likelihood).\n",
    "    frame_idx: int (0-based row index in the dataframe)\n",
    "    \"\"\"\n",
    "    # these are the crucial parts as specified in data processing instructions\n",
    "    detections_2d: np.ndarray \n",
    "    camera_projections: any # establish what type this should be, list of\n",
    "    ground_truth_3d: np.ndarray\n",
    "\n",
    "    # useful information\n",
    "    frame_idx: int # frame_id from which this comes\n",
    "#     detections_2d_file: Path # file path to filtered h5 2d data array file\n",
    "#     ground_truth_3d_file: Path # file path to 3d ground truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fce997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence:\n",
    "    def __init__(self, path: Path):\n",
    "        self.path = path\n",
    "        self.detections_2d_dir = self.path / \"filtered_2D\"\n",
    "        self.ground_truth_3d_file = self.path / \"fte_pw\" / \"fte.pickle\"\n",
    "        self.samples: List[Sample] = []\n",
    "\n",
    "\n",
    "    def load_3d_ground_truth(self):\n",
    "        # loading FTE 3D positions\n",
    "        if not self.ground_truth_3d_file.exists():\n",
    "            raise FileNotFoundError(f\"3D ground-truth file not found: {self.ground_truth_3d_file}\")\n",
    "\n",
    "        with open(self.ground_truth_3d_file, \"rb\") as f:\n",
    "            fte = pickle.load(f)\n",
    "        \n",
    "        # pickle file is a dictionary with keys dict_keys(['positions', 'x', 'dx', 'ddx', 'start_frame'])\n",
    "        # the positions is a nested list that can be converted to an array of dimension (F, J, 3)\n",
    "        # where F is number of frames, J is number of joints (25) and 3 are the 3D coordinates\n",
    "        fte_df = np.array(fte[\"positions\"], dtype=np.float32)\n",
    "        start_frame = fte[\"start_frame\"]\n",
    "\n",
    "        return fte_df, start_frame\n",
    "\n",
    "   \n",
    "    def load_2d_detections(self):\n",
    "\n",
    "        # some sequences will only have 4 cameras, but keep size of tensor as 6\n",
    "        NUM_CAMERAS = 6\n",
    "\n",
    "        cam_data: Dict[int, np.ndarray] = {}\n",
    "\n",
    "        # loop through the filtered directory\n",
    "        for file in self.detections_2d_dir.glob(\"cam*.h5\"):\n",
    "\n",
    "            # assumption: all files will begin \"camN\" where N is the camera number\n",
    "            # extract camera number from file name\n",
    "            match = re.search(r'cam(\\d+)', file.name)\n",
    "            if not match:\n",
    "                print(f\"Warning: Could not parse camera ID from {file.name}\")\n",
    "                continue\n",
    "\n",
    "            cam_id = int(match.group(1)) \n",
    "            cam_idx = cam_id - 1 # convert to 0-based index\n",
    "\n",
    "            print(f\"Loading filtered detections from {file} (camera ID {cam_id})\")\n",
    "\n",
    "            df = pd.read_hdf(file, key=\"df_with_missing\")\n",
    "\n",
    "            # drop the top level, leaving two levels (bodyparts, coords)\n",
    "            df.columns = df.columns.droplevel(0)\n",
    "\n",
    "            # can we drop the paw joints here so we have same number as ground truth\n",
    "            # also need to ensure the ordering of joints is the same\n",
    "\n",
    "\n",
    "            # todo: explain the logic of this. only slicing once per camera?\n",
    "\n",
    "            x_df = df.xs(\"x\", axis=1, level=\"coords\")\n",
    "            y_df = df.xs(\"y\", axis=1, level=\"coords\")\n",
    "            c_df = df.xs(\"likelihood\", axis=1, level=\"coords\")\n",
    "\n",
    "            x_np = x_df.to_numpy(dtype=np.float32)  \n",
    "            y_np = y_df.to_numpy(dtype=np.float32)\n",
    "            c_np = c_df.to_numpy(dtype=np.float32)\n",
    "\n",
    "            stacked = np.stack([x_np, y_np, c_np], axis=2)  # (F, J, 3)\n",
    "\n",
    "            cam_data[cam_idx] = stacked\n",
    "            \n",
    "        return cam_data\n",
    "\n",
    "        \n",
    "    def generate_samples(self):\n",
    "        \"\"\"\n",
    "        Creates self.samples using:\n",
    "          - the ground-truth timeline); for each GT frame, find the corresponding\n",
    "            row in each camera's DataFrame, build a (C, J, 3) tensor and attach GT Jx3.\n",
    "        Alignment:\n",
    "          - FTE provides a 'start_frame' value, global_frame = start_frame + gt_idx\n",
    "        \"\"\"\n",
    "        # ensure we start with a clean slate \n",
    "        self.samples = []\n",
    "\n",
    "        fte_arr, start_frame = self.load_3d_ground_truth()\n",
    "\n",
    "        # number of frames, number of joints, coordinate dimension\n",
    "        F = fte_arr.shape[0]\n",
    "        J = 25 # hardcoded but need to figure out which joints to drop to make this match the groundtruth\n",
    "        # should be:\n",
    "        # J = fte_arr.shape[1]\n",
    "        C = 6\n",
    "\n",
    "        cam_dfs = self.load_2d_detections()\n",
    "\n",
    "        for i in range(F):\n",
    "\n",
    "            global_frame = start_frame + i\n",
    "\n",
    "            # initialize base tensor, size ()\n",
    "            tensor = np.zeros((C, J, 3), dtype=np.float32)\n",
    "\n",
    "            for cam_idx in range(C):\n",
    "\n",
    "                if cam_idx not in cam_dfs:\n",
    "                    continue\n",
    "                \n",
    "                cam_df = cam_dfs[cam_idx]\n",
    "\n",
    "                # for each camera, get the positions for that frame_id\n",
    "                # what to do if camera doesn't have this frame?\n",
    "\n",
    "                row = global_frame - 1 # DLC dataframe is zero indexed, is fte pickle?\n",
    "                tensor[cam_idx, :, :] = cam_df[row]\n",
    "\n",
    "        \n",
    "            sample = Sample(\n",
    "                detections_2d=tensor, \n",
    "                camera_projections=None, # implement this, \n",
    "                ground_truth_3d = fte_arr[i],\n",
    "                frame_idx = global_frame,\n",
    "            )\n",
    "\n",
    "            self.samples.append(sample)\n",
    "        \n",
    "        return     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a97ad36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading filtered detections from /gws/nopw/j04/iecdt/cheetah/2017_08_29/bottom/phantom/flick2/filtered_2D/cam3DLC_resnet152_CheetahOct14shuffle1_500000.h5 (camera ID 3)\n",
      "Loading filtered detections from /gws/nopw/j04/iecdt/cheetah/2017_08_29/bottom/phantom/flick2/filtered_2D/cam4DLC_resnet152_CheetahOct14shuffle1_500000.h5 (camera ID 4)\n",
      "Loading filtered detections from /gws/nopw/j04/iecdt/cheetah/2017_08_29/bottom/phantom/flick2/filtered_2D/cam5DLC_resnet152_CheetahOct14shuffle1_500000.h5 (camera ID 5)\n",
      "Loading filtered detections from /gws/nopw/j04/iecdt/cheetah/2017_08_29/bottom/phantom/flick2/filtered_2D/cam6DLC_resnet152_CheetahOct14shuffle1_500000.h5 (camera ID 6)\n"
     ]
    }
   ],
   "source": [
    "test = Sequence(example_path)\n",
    "test.generate_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b5cc201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.8733630e+02, 4.9720169e+02, 3.4308180e-01],\n",
       "       [          nan,           nan, 2.7727544e-01],\n",
       "       [4.9632397e+02, 4.9722488e+02, 7.9857337e-01],\n",
       "       [5.0662256e+02, 5.1447656e+02, 8.7918854e-01],\n",
       "       [5.0698251e+02, 5.2237421e+02, 7.0071018e-01],\n",
       "       [5.0733719e+02, 5.2306757e+02, 5.5751270e-01],\n",
       "       [4.8740195e+02, 4.9704016e+02, 7.4514353e-01],\n",
       "       [4.8705948e+02, 5.0575839e+02, 8.0174422e-01],\n",
       "       [4.8734814e+02, 5.1400653e+02, 8.3344680e-01],\n",
       "       [4.8823483e+02, 5.2266437e+02, 7.7867436e-01],\n",
       "       [4.9772604e+02, 5.2270178e+02, 6.8685657e-01],\n",
       "       [4.6813193e+02, 5.1390295e+02, 9.2848915e-01],\n",
       "       [4.4818103e+02, 5.2196851e+02, 9.9610347e-01],\n",
       "       [          nan,           nan, 1.0640347e-01],\n",
       "       [          nan,           nan, 1.2626436e-01],\n",
       "       [          nan,           nan, 2.6733887e-01],\n",
       "       [          nan,           nan, 2.6833761e-01],\n",
       "       [          nan,           nan, 1.4509436e-01],\n",
       "       [          nan,           nan, 9.6028745e-02],\n",
       "       [          nan,           nan, 8.9273572e-02],\n",
       "       [          nan,           nan, 5.8835506e-02],\n",
       "       [7.8651843e+02, 5.3170374e+02, 5.7129967e-01],\n",
       "       [4.8613095e+02, 4.9724307e+02, 8.5406864e-01],\n",
       "       [          nan,           nan, 2.4705291e-01],\n",
       "       [4.8714459e+02, 4.8814362e+02, 6.6511917e-01]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example, look at first sample i.e. the first frame in the sequence\n",
    "sample0 = test.samples[0]\n",
    "sample0.ground_truth_3d # (J=20,3) array with 3d coord gt of each joint\n",
    "sample0.detections_2d # (C, J=25, 3) array for each camera at first frame (matched to the ground truth frame number)\n",
    "sample0.detections_2d[2] # look at 2d joint coordinates for camera 3 (cam index = 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
